# ecap-security-auditor ‚Äî QA Test Report

**Date:** 2026-02-02  
**Tester:** QA Subagent (Claude)  
**Perspective:** AI Agent using the skill for the first time

---

## Test 1: Automatic Security Gate ‚Äî Known Package (mcp-fetch)

**Result: ‚ö†Ô∏è PARTIAL PASS**

### Steps Performed

1. `curl -s "https://skillaudit-api.vercel.app/api/findings?package=mcp-fetch"` ‚Üí `{"findings": [], "total": 0}`
2. No report exists for "mcp-fetch" despite SKILL.md suggesting it was audited.

### Observations

- The SKILL.md says to query `/api/findings?package=PACKAGE_NAME`, but the findings for MCP servers are stored under different slugs (e.g., `mcp-server-fetch` not `mcp-fetch`).
- **An agent wouldn't know the correct slug.** There's no discovery/search endpoint to find the right name.
- The Gate Flow diagram says "Report exists? No ‚Üí Go to AUTO-AUDIT" ‚Äî but there's no actual Trust Score returned from `/api/findings`. The response only contains individual findings, no aggregate score.
- **Critical gap: The SKILL.md describes evaluating a "Trust Score" but the API never returns one.** The findings endpoint returns individual findings; the reports endpoint (for upload) accepts a `risk_score`. There's no `GET` endpoint that returns a trust score for a package.

### Decision Table Application

- Since findings=0 for mcp-fetch, an agent would interpret this as "No report exists" and trigger auto-audit ‚Äî even though `mcp-server-fetch` HAS been audited with 3 findings.

### Issues Found

1. **üî¥ BUG: No Trust Score endpoint** ‚Äî The Decision Table references scores (‚â•70, 40-69, <40) but no API endpoint returns a trust score for a package. The agent has no way to get the number.
2. **üü° Package naming ambiguity** ‚Äî No search/fuzzy-match. Agent must guess the exact slug.
3. **üü° Missing: How to derive Trust Score from findings** ‚Äî Should the agent calculate it? From what formula?

---

## Test 2: Automatic Security Gate ‚Äî Unknown Package

**Result: ‚ö†Ô∏è PARTIAL PASS**

### Steps Performed

1. `curl -s "https://skillaudit-api.vercel.app/api/findings?package=totally-unknown-package-xyz"` ‚Üí `{"findings": [], "total": 0}`

### Observations

- The response for an unknown package is **identical** to a known package with no findings (both return `total: 0`).
- An agent can't distinguish "never audited" from "audited and clean".
- The SKILL.md says to trigger AUTO-AUDIT, which is well-described in Step 4, but:
  - Step 4 says "Read ALL files in the package directory" ‚Äî but doesn't explain how to locate the package directory for npm/pip packages.
  - For npm: `node_modules/<name>/`? For pip: `site-packages/<name>/`? Not mentioned.

### Issues Found

1. **üü° Can't distinguish "unaudited" from "clean"** ‚Äî API returns same response for both.
2. **üü° Auto-audit: package location not documented** ‚Äî How does the agent find the files to audit for npm/pip packages?
3. **üü¢ Auto-audit prompt is thorough** ‚Äî `prompts/audit-prompt.md` is excellent and comprehensive.

---

## Test 3: Hash Verification

**Result: ‚úÖ PASS**

### Steps Performed

1. `bash scripts/verify.sh` ‚Üí All 6 files verified, integrity OK.
2. Tested `/api/integrity?package=totally-unknown-package-xyz` ‚Üí Returns `{"error": "Unknown package", "known_packages": ["ecap-security-auditor"]}`.

### Observations

- verify.sh works perfectly on the happy path.
- Error handling for unknown packages is clear (returns error + known packages list).
- Script is well-written: detects sha256sum/shasum, handles missing files, clear output.
- **Limitation:** verify.sh is hardcoded to `PACKAGE="ecap-security-auditor"` ‚Äî it can only verify itself. The SKILL.md Gate Flow says `bash scripts/verify.sh <package>` with an argument, but the script ignores arguments for the package name.

### Issues Found

1. **üî¥ BUG: verify.sh ignores package argument** ‚Äî Script hardcodes `PACKAGE="ecap-security-auditor"`. The SKILL.md documents usage as `bash scripts/verify.sh <package>` but the script only accepts an API URL override as `$1`, not a package name. The Gate Flow is broken for any package other than ecap-security-auditor itself.
2. **üü¢ Good:** Integrity API only knows ecap-security-auditor currently, but the script should still accept the parameter for future use.

---

## Test 4: Manual Audit Flow

**Result: ‚ö†Ô∏è PARTIAL PASS**

### Steps Performed

1. Read `prompts/audit-prompt.md` ‚Äî comprehensive and well-structured.
2. Created test report in documented JSON format.
3. Upload with `bash scripts/upload.sh /tmp/test-report.json` ‚Üí **FAILED (HTTP 400)**.

### Upload Bug Details

The SKILL.md documents this JSON format:

```json
{
  "package_name": "example-package",
  "findings": [...],
  "summary": { "risk_score": 75, "recommendation": "safe" }
}
```

But the API actually requires:

```json
{
  "skill_slug": "example-package",   // NOT package_name
  "risk_score": 75,                  // top-level, NOT in summary
  "result": "safe",                  // NOT recommendation
  "findings_count": 1,              // required, not documented
  "findings": [...]
}
```

**The documented format does NOT work.** The API error message reveals the real fields: `skill_slug (or package_name), risk_score, result, findings_count are required`.

After fixing to use `skill_slug` + top-level `risk_score` + `result` + `findings_count`, upload succeeded (Report ID: 35, Finding: ECAP-2026-0818).

### Issues Found

1. **üî¥ BUG: Documented JSON format doesn't match API** ‚Äî `package_name` works (API accepts it as alias), but `risk_score` and `result` must be top-level, not nested in `summary`. `findings_count` is required but not documented at all.
2. **üü° upload.sh doesn't validate or transform** ‚Äî The script just passes JSON through. It could map the documented format to what the API expects.
3. **üü¢ audit-prompt.md is excellent** ‚Äî Clear checklist, good false-positive guidance, severity definitions are practical.

---

## Test 5: Registration Flow

**Result: ‚úÖ PASS**

### Observations (code review only, not executed)

- Script is well-written with input validation (regex for agent name).
- Checks for existing credentials before re-registering.
- Uses `jq` to safely build JSON payload (prevents injection).
- Sets `chmod 600` on credentials file.
- Clear error messages.
- Documentation in SKILL.md is adequate.

### Issues Found

- None significant. Registration flow is solid.

---

## Test 6: API Endpoints

### GET /api/health ‚Äî ‚úÖ PASS

Returns: `{"status":"healthy","timestamp":"...","db":{"connected":true,"findings":46,"skills":13,"agents":5}}`  
Matches expected behavior. Quick and informative.

### GET /api/stats ‚Äî ‚úÖ PASS

Returns: `{"total_findings":"46","critical_findings":"2","skills_audited":"29","reporters":"1","total_reports":"34"}`  
**Minor:** Values are strings (e.g., `"46"` not `46`). Inconsistent typing but functional.

### GET /api/leaderboard ‚Äî ‚úÖ PASS

Returns array of agents with points, finding counts, timestamps. Works as documented.

### GET /api/findings?package=mcp-fetch ‚Äî ‚ö†Ô∏è ISSUE

Returns empty results. The package was audited as `mcp-server-fetch`, not `mcp-fetch`. No fuzzy matching or suggestions. See Test 1 notes.

### GET /api/findings?package=coding-agent ‚Äî ‚úÖ PASS

Returns 6 findings with full details (ecap_id, severity, title, description, file, line, etc.). Data structure is rich and useful.

### GET /api/integrity?package=ecap-security-auditor ‚Äî ‚úÖ PASS

Returns SHA-256 hashes for 6 files, repo URL, commit hash, verification timestamp. Well-structured.

### GET /api/integrity?package=totally-unknown-package-xyz ‚Äî ‚úÖ PASS (graceful error)

Returns: `{"error":"Unknown package","known_packages":["ecap-security-auditor"]}`. Helpful error.

### GET /api/agents/ecap ‚Äî ‚ö†Ô∏è ISSUE

Returns: `{"error":"Agent not found"}`.  
The SKILL.md uses `/api/agents/:name` but the registered agent is `ecap0`, not `ecap`. The docs don't clarify that the exact registered name must be used.

### GET /api/agents/ecap0 ‚Äî ‚úÖ PASS

Returns comprehensive agent profile with stats, severity breakdown, skills audited list, recent findings and reports. Excellent detail.

### Issues Found

1. **üü° /api/stats returns strings instead of numbers** ‚Äî Minor inconsistency.
2. **üü° No package search/discovery endpoint** ‚Äî Can't find the right slug for a package.
3. **üü° /api/findings doesn't return a Trust Score** ‚Äî Only raw findings. Agent must calculate score somehow.

---

## Test 7: Peer Review Flow

**Result: ‚ö†Ô∏è PARTIAL PASS**

### Observations

- `prompts/review-prompt.md` is well-written with clear verdict definitions, good/bad examples.
- The review checklist is practical and thorough.
- API endpoint for submitting reviews is documented: `POST /api/findings/:id/review`.

### Issues Found

1. **üü° Finding ID format unclear** ‚Äî The findings response returns `ecap_id` (e.g., "ECAP-2026-0777") and a numeric `id`. Which one goes in the URL path? SKILL.md uses "FINDING_ID" without clarifying. (Likely the numeric ID, but not stated.)
2. **üü° No way to list packages with findings** ‚Äî To do peer review, you need to know which packages have findings. No endpoint lists audited packages or packages needing review.
3. **üü¢ Review prompt quality is high** ‚Äî Good examples of reasoning, clear verdict criteria.

---

## Test 8: Gesamtbewertung

### Summary Table

| Test                    | Result             | Critical Issues                                                           |
| ----------------------- | ------------------ | ------------------------------------------------------------------------- |
| 1: Known Package Gate   | ‚ö†Ô∏è PARTIAL         | No Trust Score endpoint; package slug ambiguity                           |
| 2: Unknown Package Gate | ‚ö†Ô∏è PARTIAL         | Can't distinguish unaudited from clean; missing package location guidance |
| 3: Hash Verification    | ‚úÖ PASS (with bug) | verify.sh hardcoded to self only                                          |
| 4: Manual Audit         | ‚ö†Ô∏è PARTIAL         | JSON format mismatch between docs and API                                 |
| 5: Registration         | ‚úÖ PASS            | ‚Äî                                                                         |
| 6: API Endpoints        | ‚úÖ PASS (mostly)   | Stats type inconsistency; no search endpoint                              |
| 7: Peer Review          | ‚ö†Ô∏è PARTIAL         | Finding ID format unclear; no discovery                                   |
| 8: Overall              | ‚Äî                  | See below                                                                 |

### üî¥ Critical Bugs

1. **Report JSON format mismatch** ‚Äî The documented format in SKILL.md does NOT work with the API. Fields `risk_score`, `result` must be top-level; `findings_count` is required but undocumented. An agent following the docs will get HTTP 400 on every upload.

2. **verify.sh can only verify itself** ‚Äî The script hardcodes `PACKAGE="ecap-security-auditor"` and ignores the package argument. The SKILL.md Gate Flow references `bash scripts/verify.sh <package>` which doesn't work.

3. **No Trust Score endpoint** ‚Äî The entire Decision Table (Score ‚â•70/40-69/<40) references a Trust Score that no API endpoint provides. The Gate Flow is unimplementable as documented.

### üü° Medium Issues

4. Package slug discovery ‚Äî No search or fuzzy matching. Agents must guess exact slugs.
5. Can't distinguish "unaudited" vs "clean" from API response.
6. Auto-audit doesn't explain how to locate package files for npm/pip.
7. Finding ID format (numeric vs ECAP-XXXX) not clarified for review endpoint.
8. No endpoint to list packages needing review.

### üü¢ Strengths

- **audit-prompt.md is excellent** ‚Äî Comprehensive, clear false-positive guidance, good severity definitions.
- **review-prompt.md is well-crafted** ‚Äî Good examples, practical checklist.
- **Scripts are well-written** ‚Äî Good error handling, dependency checks, security practices (chmod 600, jq for JSON building).
- **API is functional and responsive** ‚Äî Fast responses, good error messages.
- **Leaderboard/points system is motivating** ‚Äî Gamification done right.
- **Overall concept is strong** ‚Äî Collaborative security auditing by AI agents is innovative.

### Verbesserungsvorschl√§ge

1. **Fix the JSON format documentation** ‚Äî Either update SKILL.md to show the actual API format, or update upload.sh to transform the documented format to what the API expects.
2. **Add a Trust Score endpoint** ‚Äî `GET /api/trust-score?package=X` returning `{"package": "X", "trust_score": 75, "findings_summary": {...}}`.
3. **Make verify.sh accept package argument** ‚Äî Change `PACKAGE="${1:-ecap-security-auditor}"` or add a second positional parameter.
4. **Add package search** ‚Äî `GET /api/search?q=fetch` returning matching package slugs.
5. **Distinguish unaudited from clean** ‚Äî Return `{"status": "unaudited"}` vs `{"findings": [], "trust_score": 100}`.
6. **Document package location** ‚Äî Add a section on where to find files for npm/pip/skill packages.
7. **Clarify Finding ID** ‚Äî State explicitly: "Use the numeric `id` field (not `ecap_id`) in API URLs."

### Gesamtnote: 6/10

**Good concept, solid tooling, but the documentation-API mismatch is a showstopper.** An agent following the SKILL.md exactly will fail at the two most important tasks: uploading reports and running the security gate. The audit/review prompts are the strongest part. Fix the three critical bugs and this becomes an 8/10.
